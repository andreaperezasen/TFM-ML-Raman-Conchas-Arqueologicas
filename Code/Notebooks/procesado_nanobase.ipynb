{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesado Espectros Raman NANOBASE\n",
    "\n",
    "En este *Notebook* se realiza un preprocesado básico a los espectros Raman medidos con Nanobase. \n",
    "\n",
    "Los datos obtenidos de NanoBase corresponden a espectros en bruto a los que se les ha restado el *background*. El programa de medición no realiza ningun otro tipo de tratamiento a los datos. Por ello es necesario realizar diferentes pasos previos al análisis de los espectros:\n",
    "\n",
    "1. Whittaker Smoother: Método de suavizado aplicado a señales con la finalidad de reducir el ruido, conservando los picos caracteristicos del material. El método funciona minimizando la suma de las diferencias al cuadrado entre cada punto de datos y su estimación suavizada, sujeto a una restricción de que la segunda diferencia de los valores suavizados es pequeña. Esta restricción tiene el efecto de penalizar las variaciones en la tasa de cambio de los valores suavizados, resultando en una señal más suave.\n",
    "\n",
    "2. Baseline: \n",
    "\n",
    "Al contrario que en el preprocesado de la Sonda Raman de 532nm, no es necesario corregir el Background ya que el propio software de medida se encarga de hacerlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de archivos CSV:  9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#%matplotlib inline\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import time\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, Normalizer\n",
    "from scipy.linalg import solveh_banded  # para hacer ALS\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.signal import argrelextrema\n",
    "from itertools import product\n",
    "from scipy.signal import savgol_filter\n",
    "from functools import reduce\n",
    "\n",
    "# Directorio en el que se encuentran guardados los datos de Raman\n",
    "path_nanobase_mac = '/Users/drea/Library/CloudStorage/OneDrive-UNICAN/RamanNanoBase/CONCHAS_ARQ/CSV'\n",
    "path_nanobase_windows ='D:\\OneDrive - UNICAN\\RamanNanoBase\\CONCHAS_ARQ\\CSV'\n",
    "# Elegimos la ruta según el sistema operativo\n",
    "path_nanobase = path_nanobase_windows\n",
    "# Obtén la lista de archivos en la carpeta\n",
    "files = os.listdir(path_nanobase)\n",
    "# Filtrar solo los archivos CSV\n",
    "csv_files = [f for f in files if f.endswith('.CSV')]\n",
    "\n",
    "print(\"Número de archivos CSV: \", len(csv_files))\n",
    "\n",
    "# Copiamos los archivos a la carpeta contenedora. Esto se realiza para poder actualizar las nuevas medidas\n",
    "def copiar_archivos(origen, destino, archivos):\n",
    "    # Copiar cada archivo seleccionado a la carpeta de destino si no existe\n",
    "    for archivo in archivos:\n",
    "        ruta_origen = os.path.join(origen, archivo)\n",
    "        ruta_destino = os.path.join(destino, archivo)\n",
    "        if os.path.exists(ruta_origen):\n",
    "            shutil.copy2(ruta_origen, ruta_destino)\n",
    "            print(f\"Archivo {archivo} copiado correctamente.\")\n",
    "        else:\n",
    "            print(f\"El archivo {archivo} no existe en la carpeta de origen.\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "carpeta_origen = 'D:\\OneDrive - UNICAN\\RamanNanoBase\\CONCHAS_ARQ\\CSV'\n",
    "carpeta_destino = \"D:\\OneDrive - UNICAN\\Escritorio\\Conchas\\CSV_nanobase\"\n",
    "\n",
    "# copiar_archivos(carpeta_origen, carpeta_destino, csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de archivos CSV:  9\n"
     ]
    }
   ],
   "source": [
    "# Directorios de medida\n",
    "csv_files = os.listdir(carpeta_destino)\n",
    "print(\"Número de archivos CSV: \", len(csv_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhittakerSmoother(object):\n",
    "    def __init__(self, signal, smoothness_param, deriv_order=1):\n",
    "        self.y = signal\n",
    "        assert deriv_order > 0, 'deriv_order must be an int > 0'\n",
    "        # Compute the fixed derivative of identity (D).\n",
    "        d = np.zeros(deriv_order*2 + 1, dtype=int)\n",
    "        d[deriv_order] = 1\n",
    "        d = np.diff(d, n=\n",
    "        deriv_order)\n",
    "        n = self.y.shape[0]\n",
    "        k = len(d)\n",
    "        s = float(smoothness_param)\n",
    "        # Here be dragons: essentially we're faking a big banded matrix D,\n",
    "        # doing s * D.T.dot(D) with it, then taking the upper triangular bands.\n",
    "        diag_sums = np.vstack([\n",
    "            np.pad(s*np.cumsum(d[-i:]*d[:i]), ((k-i,0),), 'constant')\n",
    "            for i in range(1, k+1)])\n",
    "        upper_bands = np.tile(diag_sums[:,-1:], n)\n",
    "        upper_bands[:,:k] = diag_sums\n",
    "        for i,ds in enumerate(diag_sums):\n",
    "            upper_bands[i,-i-1:] = ds[::-1][:i+1]\n",
    "        self.upper_bands = upper_bands\n",
    "    def smooth(self, w):\n",
    "        foo = self.upper_bands.copy()\n",
    "        foo[-1] += w  # last row is the diagonal\n",
    "        return solveh_banded(foo, w * self.y, overwrite_ab=True, overwrite_b=True)\n",
    "\n",
    "def als_baseline(intensities, asymmetry_param=0.0001, smoothness_param=1e4,\n",
    "                 max_iters=20, conv_thresh=1e-6, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies the asymmetric least squares (ALS) method to fit the baseline.\n",
    "\n",
    "    Parameters:\n",
    "    - intensities (numpy.ndarray or pandas.Series): Numpy array or pandas Series representing the Raman intensities.\n",
    "    - asymmetry_param (float): Asymmetry parameter for the fit.\n",
    "    - smoothness_param (float): Smoothness parameter for the fit.\n",
    "    - max_iters (int): Maximum number of iterations.\n",
    "    - conv_thresh (float): Convergence threshold.\n",
    "    - verbose (bool): Flag to print debugging information.\n",
    "\n",
    "    Returns:\n",
    "    - baseline (numpy.ndarray): Numpy array representing the fitted baseline.\n",
    "    \"\"\"\n",
    "    if isinstance(intensities, pd.Series):\n",
    "        intensities = intensities.values\n",
    "\n",
    "    smoother = WhittakerSmoother(intensities, smoothness_param, deriv_order=2)\n",
    "    p = asymmetry_param\n",
    "    w = np.ones(intensities.shape[0])\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        z = smoother.smooth(w)\n",
    "        mask = intensities > z\n",
    "        new_w = p * mask + (1 - p) * (~mask)\n",
    "        conv = np.linalg.norm(new_w - w)\n",
    "        if verbose:\n",
    "            print(i + 1, conv)\n",
    "        if conv < conv_thresh:\n",
    "            break\n",
    "        w = new_w\n",
    "    else:\n",
    "        None\n",
    "        #print('ALS did not converge in %d iterations' % max_iters)\n",
    "    return z\n",
    "\n",
    "def subtract(df, df_background):\n",
    "    df_subtracted = df.sub(df_background.iloc[0], axis=1)\n",
    "    return df_subtracted\n",
    "\n",
    "def find_baseline(df, asymmetry_param=0.0001, smoothness_param=1e4,\n",
    "                 max_iters=20, conv_thresh=1e-6):\n",
    "    \"\"\"\n",
    "    Apply baseline correction to each row in a DataFrame.\n",
    "    Input:\n",
    "        df: DataFrame, contains the spectral data.\n",
    "    Output:\n",
    "        DataFrame, with baseline corrected for each spectrum.\n",
    "    \"\"\"\n",
    "    D0_BL = df.copy()\n",
    "    for i in range(len(df)):\n",
    "        # Apply ALS baseline correction to each row\n",
    "        bl = als_baseline(df.iloc[i, :].values, asymmetry_param, smoothness_param,\n",
    "                 max_iters, conv_thresh)\n",
    "        D0_BL.iloc[i, :] = bl\n",
    "    return D0_BL\n",
    "\n",
    "def correct_baseline(data, baseline):\n",
    "    \"\"\"\n",
    "    Corrects the baseline of the data.\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame or pandas.Series): DataFrame or Series containing the data.\n",
    "    - baseline (pandas.DataFrame or pandas.Series): DataFrame or Series with the baselines.\n",
    "    Returns:\n",
    "    - corrected_data (pandas.DataFrame): DataFrame with the corrected data.\n",
    "    \"\"\"\n",
    "    # Ensure that both data and baseline are DataFrames\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "         data = pd.DataFrame(data)\n",
    "    if not isinstance(baseline, pd.DataFrame):\n",
    "         baseline = pd.DataFrame(baseline)\n",
    "    # Perform baseline correction\n",
    "    \n",
    "    corrected_data = data - baseline\n",
    "\n",
    "    return corrected_data\n",
    "\n",
    "\n",
    "def normalize_spectra(X_filtered):\n",
    "    \"\"\"\n",
    "    Preprocessing of spectral data.\n",
    "    Input:\n",
    "        X_filtered: DataFrame, raw spectral data.\n",
    "    Output:\n",
    "        DataFrame, preprocessed spectral data.\n",
    "    \"\"\"\n",
    "    X_norm_list = []\n",
    "    for _, row in X_filtered.iterrows():\n",
    "        # Normalize each spectrum by dividing by the sum and multiplying by a constant\n",
    "        total = np.sum(row)\n",
    "        X_norm_list.append(pd.DataFrame([row / total * 2000]))\n",
    "    X_norm = pd.concat(X_norm_list, ignore_index=True)\n",
    "    # Apply Savitzky-Golay filter for smoothing\n",
    "    X_norm_flt = savgol_filter(X_norm, 21, 2)\n",
    "    X_norm_flt = pd.DataFrame(X_norm_flt, columns=X_filtered.columns)\n",
    "    '''# Perform baseline correction\n",
    "    bl = find_baseline(X_norm_flt)\n",
    "    X_norm_flt = X_norm_flt - bl'''\n",
    "    # Standardize the data using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    z_numpy = X_norm_flt.values\n",
    "    z_numpy_scaled_rows = scaler.fit_transform(z_numpy.T).T\n",
    "    X_norm_flt_stdz = pd.DataFrame(data=z_numpy_scaled_rows, columns=X_filtered.columns)\n",
    "    return X_norm_flt_stdz\n",
    "\n",
    "def SNV(input_data):\n",
    "    \"\"\"\n",
    "    Procesado de datos: SNV (Standard Normal Variate).\n",
    "    1) Media de cada channel --> axis=1 (todas las columnas).\n",
    "    2) Se le resta su media a cada \"channel\" --> axis=0 (todas las filas).\n",
    "    3) Divide cada channel por su StDev --> axis=1 (todas las columnas).\n",
    "    :parameter *input_data*: matriz de datos, en formato: \"channels(f) x time(c)\".\n",
    "    :return: matriz de datos, una vez normalizados\n",
    "    \"\"\"\n",
    "    data_12 = input_data.sub(input_data.mean(axis=1), axis=0)\n",
    "    data_snv = data_12.div(input_data.std(axis=1), axis=0)\n",
    "    return data_snv\n",
    "\n",
    "def cut_spectrum(df, min_wavelength, max_wavelength):\n",
    "    \"\"\"\n",
    "    Cuts the spectrum to a specific range.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): DataFrame containing the spectrum.\n",
    "    - min_wavelength (int): Minimum wavelength.\n",
    "    - max_wavelength (int): Maximum wavelength.\n",
    "    Returns:\n",
    "    - df_cut (pandas.DataFrame): DataFrame containing the spectrum within the specified range.\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.astype(float) #si tus columnas deben ser números flotantes\n",
    "    df_cut = df.loc[:, (df.columns >= min_wavelength) & (df.columns <= max_wavelength)]\n",
    "    return df_cut\n",
    "\n",
    "def cut_spectrum_2(df, min_wavelength, max_wavelength):\n",
    "    \"\"\"\n",
    "    Cuts the spectrum to a specific range.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): DataFrame containing the spectrum.\n",
    "    - min_wavelength (int): Minimum wavelength to include in the cut.\n",
    "    - max_wavelength (int): Maximum wavelength to include in the cut.\n",
    "    Returns:\n",
    "    - df_cut (pandas.DataFrame): DataFrame containing the spectrum within the specified range.\n",
    "    \"\"\"\n",
    "    # Filter out non-numeric columns before attempting to convert column names to float\n",
    "    numeric_columns = [col for col in df.columns if col.replace('.', '', 1).isdigit()]\n",
    "    df_numeric = df[numeric_columns]\n",
    "\n",
    "    # Convert numeric column names to float\n",
    "    df_numeric.columns = df_numeric.columns.astype(float)\n",
    "\n",
    "    # Cut the spectrum to the specified range\n",
    "    df_cut = df_numeric.loc[:, (df_numeric.columns >= min_wavelength) & (df_numeric.columns <= max_wavelength)]\n",
    "\n",
    "    return df_cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhittakerSmoother(object):\n",
    "    def __init__(self, signal, smoothness_param, deriv_order=1):\n",
    "        self.y = signal\n",
    "        assert deriv_order > 0, 'deriv_order must be an int > 0'\n",
    "        # Compute the fixed derivative of identity (D).\n",
    "        d = np.zeros(deriv_order*2 + 1, dtype=int)\n",
    "        # d[deriv_order] = 1\n",
    "        d = np.diff(d, n=\n",
    "        deriv_order)\n",
    "        n = self.y.shape[0]\n",
    "        k = len(d)\n",
    "        s = float(smoothness_param)\n",
    "        # Here be dragons: essentially we're faking a big banded matrix D,\n",
    "        # doing s * D.T.dot(D) with it, then taking the upper triangular bands.\n",
    "        diag_sums = np.vstack([\n",
    "            np.pad(s*np.cumsum(d[-i:]*d[:i]), ((k-i,0),), 'constant')\n",
    "            for i in range(1, k+1)])\n",
    "        upper_bands = np.tile(diag_sums[:,-1:], n)\n",
    "        upper_bands[:,:k] = diag_sums\n",
    "        for i,ds in enumerate(diag_sums):\n",
    "            upper_bands[i,-i-1:] = ds[::-1][:i+1]\n",
    "        self.upper_bands = upper_bands\n",
    "    def smooth(self, w):\n",
    "        foo = self.upper_bands.copy()\n",
    "        foo[-1] += w  # last row is the diagonal\n",
    "        return solveh_banded(foo, w * self.y, overwrite_ab=True, overwrite_b=True)\n",
    "\n",
    "def als_baseline(intensities, asymmetry_param=0.0001, smoothness_param=1e4,\n",
    "                 max_iters=20, conv_thresh=1e-6, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies the asymmetric least squares (ALS) method to fit the baseline.\n",
    "\n",
    "    Parameters:\n",
    "    - intensities (numpy.ndarray or pandas.Series): Numpy array or pandas Series representing the Raman intensities.\n",
    "    - asymmetry_param (float): Asymmetry parameter for the fit.\n",
    "    - smoothness_param (float): Smoothness parameter for the fit.\n",
    "    - max_iters (int): Maximum number of iterations.\n",
    "    - conv_thresh (float): Convergence threshold.\n",
    "    - verbose (bool): Flag to print debugging information.\n",
    "\n",
    "    Returns:\n",
    "    - baseline (numpy.ndarray): Numpy array representing the fitted baseline.\n",
    "    \"\"\"\n",
    "    if isinstance(intensities, pd.Series):\n",
    "        intensities = intensities.values\n",
    "\n",
    "    smoother = WhittakerSmoother(intensities, smoothness_param, deriv_order=2)\n",
    "    p = asymmetry_param\n",
    "    w = np.ones(intensities.shape[0])\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        z = smoother.smooth(w)\n",
    "        mask = intensities > z\n",
    "        new_w = p * mask + (1 - p) * (~mask)\n",
    "        conv = np.linalg.norm(new_w - w)\n",
    "\n",
    "        if verbose:\n",
    "            print(i + 1, conv)\n",
    "\n",
    "        if conv < conv_thresh:\n",
    "            break\n",
    "\n",
    "        w = new_w\n",
    "    else:\n",
    "        # print('ALS did not converge in %d iterations' % max_iters)\n",
    "        pass\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "def find_baseline(df):\n",
    "    \"\"\"\n",
    "    Apply baseline correction to each row in a DataFrame.\n",
    "    Input:\n",
    "        df: DataFrame, contains the spectral data.\n",
    "    Output:\n",
    "        DataFrame, with baseline corrected for each spectrum.\n",
    "    \"\"\"\n",
    "    D0_BL = df.copy()\n",
    "    for i in range(len(df)):\n",
    "        # Apply ALS baseline correction to each row\n",
    "        bl = als_baseline(df.iloc[i, :].values)\n",
    "        D0_BL.iloc[i, :] = bl\n",
    "    return D0_BL\n",
    "\n",
    "def remove_background(df):\n",
    "    D0_BG = df.copy()\n",
    "\n",
    "    return D0_BG\n",
    "\n",
    "def correct_baseline(data, baseline):\n",
    "    \"\"\"\n",
    "    Corrects the baseline of the data.\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame or pandas.Series): DataFrame or Series containing the data.\n",
    "    - baseline (pandas.DataFrame or pandas.Series): DataFrame or Series with the baselines.\n",
    "    Returns:\n",
    "    - corrected_data (pandas.DataFrame): DataFrame with the corrected data.\n",
    "    \"\"\"\n",
    "    # Ensure that both data and baseline are DataFrames\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    if not isinstance(baseline, pd.DataFrame):\n",
    "        baseline = pd.DataFrame(baseline)\n",
    "\n",
    "    # Perform baseline correction\n",
    "    corrected_data = data.values - baseline.values\n",
    "\n",
    "    # Convert the result back to a DataFrame while maintaining the same column names\n",
    "    corrected_data = pd.DataFrame(corrected_data, columns=data.columns).copy()\n",
    "\n",
    "    return corrected_data\n",
    "\n",
    "\n",
    "def normalize_spectra(X_filtered):\n",
    "    \"\"\"\n",
    "    Preprocessing of spectral data.\n",
    "    Input:\n",
    "        X_filtered: DataFrame, raw spectral data.\n",
    "    Output:\n",
    "        DataFrame, preprocessed spectral data.\n",
    "    \"\"\"\n",
    "    X_norm_list = []\n",
    "    for _, row in X_filtered.iterrows():\n",
    "        # Normalize each spectrum by dividing by the sum and multiplying by a constant\n",
    "        total = np.sum(row)\n",
    "        X_norm_list.append(pd.DataFrame([row / total * 2000]))\n",
    "\n",
    "    X_norm = pd.concat(X_norm_list, ignore_index=True)\n",
    "\n",
    "    # Apply Savitzky-Golay filter for smoothing\n",
    "    X_norm_flt = savgol_filter(X_norm, 21, 2)\n",
    "    X_norm_flt = pd.DataFrame(X_norm_flt, columns=X_filtered.columns)\n",
    "\n",
    "    '''# Perform baseline correction\n",
    "    bl = find_baseline(X_norm_flt)\n",
    "    X_norm_flt = X_norm_flt - bl'''\n",
    "\n",
    "    # Standardize the data using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    z_numpy = X_norm_flt.values\n",
    "    z_numpy_scaled_rows = scaler.fit_transform(z_numpy.T).T\n",
    "    X_norm_flt_stdz = pd.DataFrame(data=z_numpy_scaled_rows, columns=X_filtered.columns)\n",
    "\n",
    "    return X_norm_flt_stdz\n",
    "\n",
    "\n",
    "\n",
    "def procesarRamanNanobase(file):\n",
    "    \n",
    "    df = pd.read_csv(file, engine='python', sep=',', skiprows=14, decimal='.')\n",
    "    resultados = []\n",
    "    archivo= file\n",
    "\n",
    "    # # Drop unwanted column\n",
    "    if 'Unnamed: 1937' in df.columns:\n",
    "            df = df.drop(columns=['Unnamed: 1937'])\n",
    "\n",
    "    # Store coordinates\n",
    "    coordenadas = df.iloc[:, 0:2].T\n",
    "        \n",
    "    # Select only ramanshift columns\n",
    "    df2 = df.iloc[:, 7:]\n",
    "\n",
    "    # Create a DataFrame for Raman shift values\n",
    "    ramanshitfs = pd.DataFrame(df2.columns.astype(float), columns=['Raman shift'])\n",
    "\n",
    "    # Select Raman spectra data within the desired range\n",
    "    D = df2.loc[:, (df2.columns.astype(float) >= 500) & (df2.columns.astype(float) <=2000)]\n",
    "\n",
    "    # Filter Raman shift values within the desired range\n",
    "    ramanshitfs = ramanshitfs.loc[(ramanshitfs['Raman shift'] >= 500) & (ramanshitfs['Raman shift'] <= 2000)]\n",
    "\n",
    "\n",
    "    #Filtrado de espectros saturados\n",
    "    # guardado del número total de filas\n",
    "    total_rows = len(D)\n",
    "    # Descartar las filas donde cualquier valor supera 65000\n",
    "    D = D[~(D > 65000).any(axis=1)]\n",
    "\n",
    "    # Guardar el número de filas después de descartar\n",
    "    remaining_rows = len(D)\n",
    "    # Calcular e imprimir el número de filas descartadas\n",
    "    discarded_rows = total_rows - remaining_rows\n",
    "    print(f'{archivo}: se descartaron {discarded_rows} espectros saturados de un total de {total_rows} espectros.')\n",
    "    \n",
    "    # Find baseline of Raman spectra data\n",
    "    D_BL = find_baseline(D)\n",
    "    # Correct baseline of Raman spectra data\n",
    "    D_BC =correct_baseline(D, D_BL)\n",
    "    # Normalize Raman spectra data\n",
    "    D_PP = normalize_spectra(D_BC)\n",
    "\n",
    "    df_final = pd.concat([coordenadas.T, D_PP], axis=1)\n",
    "    df_final.set_index(['X', 'Y'], inplace=True)\n",
    "\n",
    "    tipo_laser = \"Nanobase\"\n",
    "    nombre_archivo = archivo.split('\\\\')[-1].split('.')[0]\n",
    "    nombre_muestra = nombre_archivo.split('_')[1]\n",
    "    fecha_medida = nombre_archivo.split('_')[0]\n",
    "    hora_medida = nombre_archivo.split(\"_\")[0].split(\"-\")[1]\n",
    "    tipo = 'raw'\n",
    "    df_final.index = pd.MultiIndex.from_tuples([( tipo_laser, nombre_muestra, tipo,\n",
    "                                                        fecha_medida,hora_medida, nombre_archivo, X, Y, ) for X, Y in df_final.index],\n",
    "                                                  names=[ \"Instrumento\",\"Muestra\",\"Tipo\",  \"Fecha\",\"Hora\", \"Archivo\",\"X\", \"Y\"])\n",
    "    \n",
    "    # Append processed data to the results list\n",
    "    resultados.append((archivo, coordenadas, D, D_BL, D_PP, ramanshitfs))\n",
    "\n",
    "    return df_final, resultados\n",
    "\n",
    "\n",
    "def procesarRamanSonda(file):\n",
    "    resultados_sonda =[]\n",
    "    archivo = file\n",
    "\n",
    "    df_sonda = pd.read_csv(file)\n",
    "    # Store coordinates\n",
    "    df_sonda_reset = df_sonda.reset_index()\n",
    "    coordenadas_sonda = df_sonda_reset.iloc[:, :2]\n",
    "    coordenadas_sonda.columns = ['X', 'Y']\n",
    "    coordenadas_sonda = coordenadas_sonda.T\n",
    "            \n",
    "    df2_sonda = df_sonda.iloc[:, 3:]\n",
    "\n",
    "    # Create a DataFrame for Raman shift values\n",
    "    ramanshitfs_sonda = pd.DataFrame(df2_sonda.columns.astype(float), columns=['Raman shift'])\n",
    "    # Select Raman spectra data within the desired range\n",
    "    D_sonda = df2_sonda.loc[:, (df2_sonda.columns.astype(float) >= 500.0) & (df2_sonda.columns.astype(float) <= 2000.0)]\n",
    "    # Filter Raman shift values within the desired range\n",
    "    ramanshitfs_sonda = ramanshitfs_sonda.loc[(ramanshitfs_sonda['Raman shift'] >= 500) & (ramanshitfs_sonda['Raman shift'] <= 2000)]\n",
    "    #Filtrado de espectros saturados\n",
    "    total_rows_sonda = len(D_sonda)\n",
    "    # Descartar las filas donde cualquier valor supera 65000\n",
    "    D_sonda = D_sonda[~(D_sonda > 65000).any(axis=1)]\n",
    "    # Guardar el número de filas después de descartar\n",
    "    remaining_rows_sonda = len(D_sonda)\n",
    "    # Calcular e imprimir el número de filas descartadas\n",
    "    discarded_rows_sonda = total_rows_sonda - remaining_rows_sonda\n",
    "\n",
    "    archivo_sonda ='Sonda'\n",
    "    print(f'{archivo_sonda}: se descartaron {discarded_rows_sonda} espectros saturados de un total de {total_rows_sonda} espectros.')\n",
    "\n",
    "        # Removing Background\n",
    "    D_BG_sonda = remove_background(D_sonda)\n",
    "    # Find baseline of Raman spectra data\n",
    "    D_BL_sonda = find_baseline(D_BG_sonda)\n",
    "    # Correct baseline of Raman spectra data\n",
    "    D_BC_sonda = correct_baseline(D_sonda, D_BL_sonda)\n",
    "    # Normalize Raman spectra data\n",
    "    D_PP_sonda = normalize_spectra(D_BC_sonda)\n",
    "\n",
    "    \n",
    "    df_sonda_final = pd.concat([coordenadas_sonda.T, D_PP_sonda], axis=1)\n",
    "    df_sonda_final.set_index(['X', 'Y'], inplace=True)\n",
    "\n",
    "    resultados_sonda.append((archivo, coordenadas_sonda, D_sonda, D_BL_sonda, D_PP_sonda, ramanshitfs_sonda))\n",
    "\n",
    "    nombre_archivo = archivo.split(\"\\\\\")[-2] # Nombre del archivo\n",
    "    nombre_muestra = nombre_archivo.split(\"_\")[3] # Nombre de la muestra\n",
    "    fecha_medida = nombre_archivo.split(\"_\")[0].split(\"-\")[0]  # Fecha de la medida\n",
    "    hora_medida = nombre_archivo.split(\"_\")[0].split(\"-\")[1]    # Hora de la medida\n",
    "    tipo_laser =   f\"Sonda {nombre_archivo.split('_')[2]}\"\n",
    "    tipo = 'raw'\n",
    "    df_sonda_final.index = pd.MultiIndex.from_tuples([( tipo_laser, nombre_muestra, tipo,\n",
    "                                                        fecha_medida,hora_medida, nombre_archivo, X, Y, ) for X, Y in df_sonda_final.index],\n",
    "                                                  names=[ \"Instrumento\",\"Muestra\",\"Tipo\",  \"Fecha\",\"Hora\", \"Archivo\",\"X\", \"Y\"])\n",
    "    return df_sonda_final, resultados_sonda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhittakerSmoother(object):\n",
    "  def __init__(self, signal, smoothness_param, deriv_order=1):\n",
    "    self.y = signal\n",
    "    assert deriv_order > 0, 'deriv_order must be an int > 0'\n",
    "    # Compute the fixed derivative of identity (D).\n",
    "    d = np.zeros(deriv_order*2 + 1, dtype=int)\n",
    "    d[deriv_order] = 1\n",
    "    d = np.diff(d, n=\n",
    "    deriv_order)\n",
    "    n = self.y.shape[0]\n",
    "    k = len(d)\n",
    "    s = float(smoothness_param)\n",
    "    # Here be dragons: essentially we're faking a big banded matrix D,\n",
    "    # doing s * D.T.dot(D) with it, then taking the upper triangular bands.\n",
    "    diag_sums = np.vstack([\n",
    "        np.pad(s*np.cumsum(d[-i:]*d[:i]), ((k-i,0),), 'constant')\n",
    "        for i in range(1, k+1)])\n",
    "    upper_bands = np.tile(diag_sums[:,-1:], n)\n",
    "    upper_bands[:,:k] = diag_sums\n",
    "    for i,ds in enumerate(diag_sums):\n",
    "      upper_bands[i,-i-1:] = ds[::-1][:i+1]\n",
    "    self.upper_bands = upper_bands\n",
    "  def smooth(self, w):\n",
    "    foo = self.upper_bands.copy()\n",
    "    foo[-1] += w  # last row is the diagonal\n",
    "    return solveh_banded(foo, w * self.y, overwrite_ab=True, overwrite_b=True)\n",
    "\n",
    "\n",
    "def als_baseline(intensities, asymmetry_param=0.0001, smoothness_param=1e4,\n",
    "                 max_iters=20, conv_thresh=1e-6, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies the asymmetric least squares (ALS) method to fit the baseline.\n",
    "\n",
    "    Parameters:\n",
    "    - intensities (numpy.ndarray or pandas.Series): Numpy array or pandas Series representing the Raman intensities.\n",
    "    - asymmetry_param (float): Asymmetry parameter for the fit.\n",
    "    - smoothness_param (float): Smoothness parameter for the fit.\n",
    "    - max_iters (int): Maximum number of iterations.\n",
    "    - conv_thresh (float): Convergence threshold.\n",
    "    - verbose (bool): Flag to print debugging information.\n",
    "\n",
    "    Returns:\n",
    "    - baseline (numpy.ndarray): Numpy array representing the fitted baseline.\n",
    "    \"\"\"\n",
    "    if isinstance(intensities, pd.Series):\n",
    "        intensities = intensities.values\n",
    "\n",
    "    smoother = WhittakerSmoother(intensities, smoothness_param, deriv_order=2)\n",
    "    p = asymmetry_param\n",
    "    w = np.ones(intensities.shape[0])\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        z = smoother.smooth(w)\n",
    "        mask = intensities > z\n",
    "        new_w = p * mask + (1 - p) * (~mask)\n",
    "        conv = np.linalg.norm(new_w - w)\n",
    "\n",
    "        if verbose:\n",
    "            print(i + 1, conv)\n",
    "\n",
    "        if conv < conv_thresh:\n",
    "            break\n",
    "\n",
    "        w = new_w\n",
    "    else:\n",
    "        print('ALS did not converge in %d iterations' % max_iters)\n",
    "\n",
    "    return z\n",
    "\n",
    "def find_baseline(df):\n",
    "    \"\"\"\n",
    "    Apply baseline correction to each row in a DataFrame.\n",
    "    Input:\n",
    "        df: DataFrame, contains the spectral data.\n",
    "    Output:\n",
    "        DataFrame, with baseline corrected for each spectrum.\n",
    "    \"\"\"\n",
    "    D0_BL = df.copy()\n",
    "    for i in range(len(df)):\n",
    "        # Apply ALS baseline correction to each row\n",
    "        bl = als_baseline(df.iloc[i, :].values)\n",
    "        D0_BL.iloc[i, :] = bl\n",
    "    return D0_BL\n",
    "\n",
    "\n",
    "def correct_baseline(data, baseline):\n",
    "    \"\"\"\n",
    "    Corrects the baseline of the data.\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame or pandas.Series): DataFrame or Series containing the data.\n",
    "    - baseline (pandas.DataFrame or pandas.Series): DataFrame or Series with the baselines.\n",
    "    Returns:\n",
    "    - corrected_data (pandas.DataFrame): DataFrame with the corrected data.\n",
    "    \"\"\"\n",
    "    # Ensure that both data and baseline are DataFrames\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    if not isinstance(baseline, pd.DataFrame):\n",
    "        baseline = pd.DataFrame(baseline)\n",
    "\n",
    "    # Perform baseline correction\n",
    "    corrected_data = data.values - baseline.values\n",
    "\n",
    "    # Convert the result back to a DataFrame while maintaining the same column names\n",
    "    corrected_data = pd.DataFrame(corrected_data, columns=data.columns).copy()\n",
    "\n",
    "    return corrected_data\n",
    "\n",
    "\n",
    "def normalize_spectra(X_filtered):\n",
    "    \"\"\"\n",
    "    Preprocessing of spectral data.\n",
    "    Input:\n",
    "        X_filtered: DataFrame, raw spectral data.\n",
    "    Output:\n",
    "        DataFrame, preprocessed spectral data.\n",
    "    \"\"\"\n",
    "    X_norm_list = []\n",
    "    for _, row in X_filtered.iterrows():\n",
    "        # Normalize each spectrum by dividing by the sum and multiplying by a constant\n",
    "        total = np.sum(row)\n",
    "        X_norm_list.append(pd.DataFrame([row / total * 2000]))\n",
    "\n",
    "    X_norm = pd.concat(X_norm_list, ignore_index=True)\n",
    "\n",
    "    # Apply Savitzky-Golay filter for smoothing\n",
    "    X_norm_flt = savgol_filter(X_norm, 21, 2)\n",
    "    X_norm_flt = pd.DataFrame(X_norm_flt, columns=X_filtered.columns)\n",
    "\n",
    "    '''# Perform baseline correction\n",
    "    bl = find_baseline(X_norm_flt)\n",
    "    X_norm_flt = X_norm_flt - bl'''\n",
    "\n",
    "    # Standardize the data using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    z_numpy = X_norm_flt.values\n",
    "    z_numpy_scaled_rows = scaler.fit_transform(z_numpy.T).T\n",
    "    X_norm_flt_stdz = pd.DataFrame(data=z_numpy_scaled_rows, columns=X_filtered.columns)\n",
    "\n",
    "    return X_norm_flt_stdz\n",
    "\n",
    "\n",
    "def procesarRaman_carpeta(archivos_csv):\n",
    "    \"\"\"\n",
    "    Process Raman spectra data from a folder.\n",
    "\n",
    "    Args:\n",
    "        ruta (str): The path to the folder containing the Raman spectra data files.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing the processed data for each file. Each tuple contains:\n",
    "            - The file name\n",
    "            - The coordinates\n",
    "            - The raw Raman spectra data\n",
    "            - The baseline-corrected Raman spectra data\n",
    "            - The preprocessed Raman spectra data\n",
    "            - The Raman shift values\n",
    "    \"\"\"\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for archivo in archivos_csv:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(os.path.join('D:\\OneDrive - UNICAN\\Escritorio\\Conchas\\CSV_nanobase', archivo), engine='python', sep=',', skiprows=14, decimal='.')\n",
    "\n",
    "        # Drop unwanted column\n",
    "        if 'Unnamed: 1937' in df.columns:\n",
    "            df = df.drop(columns=['Unnamed: 1937'])\n",
    "\n",
    "        # Store coordinates\n",
    "        coordenadas = df.iloc[:, 0:2].T\n",
    "        \n",
    "        # Select only ramanshift columns\n",
    "        df2 = df.iloc[:, 7:]\n",
    "\n",
    "        # Create a DataFrame for Raman shift values\n",
    "        ramanshitfs = pd.DataFrame(df2.columns.astype(float), columns=['Raman shift'])\n",
    "\n",
    "        # Select Raman spectra data within the desired range\n",
    "        D = df2.loc[:, (df2.columns.astype(float) >= 600.0) & (df2.columns.astype(float) <= 2800.0)]\n",
    "\n",
    "        # Filter Raman shift values within the desired range\n",
    "        ramanshitfs = ramanshitfs.loc[(ramanshitfs['Raman shift'] >= 600) & (ramanshitfs['Raman shift'] <= 2800)]\n",
    "\n",
    "\n",
    "        #Filtrado de espectros saturados\n",
    "        # guardado del número total de filas\n",
    "        total_rows = len(D)\n",
    "        # Descartar las filas donde cualquier valor supera 65000\n",
    "        D = D[~(D > 65000).any(axis=1)]\n",
    "\n",
    "        # Guardar el número de filas después de descartar\n",
    "        remaining_rows = len(D)\n",
    "        # Calcular e imprimir el número de filas descartadas\n",
    "        discarded_rows = total_rows - remaining_rows\n",
    "        print(f'{archivo}: se descartaron {discarded_rows} espectros saturados de un total de {total_rows} espectros.')\n",
    "    \n",
    "\n",
    "        # Find baseline of Raman spectra data\n",
    "        D_BL = find_baseline(D)\n",
    "\n",
    "        # Correct baseline of Raman spectra data\n",
    "        D_BC = correct_baseline(D, D_BL)\n",
    "\n",
    "        # Normalize Raman spectra data\n",
    "        # D_PP = normalize_spectra(D_BC)\n",
    "        D_PP = D_BC\n",
    "\n",
    "        # Append processed data to the results list\n",
    "        resultados.append((archivo, coordenadas, D, D_BC, ramanshitfs))\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['160424_LIT4_1000ms_n5_potenica1c_mapa_abajo.CSV',\n",
       " '170424_LIT2_1000ms_n5_potencia1c_mapa_abajo.CSV',\n",
       " '180424_LIT1_1000ms_n5_potencia1c_mapa_abajo.CSV',\n",
       " '180424_LIT3_1000ms_n5_potencia1c_mapa_abajo.CSV',\n",
       " '190424_LIT5_1000ms_n10_potencia1c_abajo.CSV',\n",
       " '230424_LIT6_1000ms_n5_potencia1c_abajo.CSV',\n",
       " '240424_LIT235.3_2000ms_n10_potencia1c_abajo.CSV',\n",
       " '250424_LIT885.1_3000ms_n8_potencia1c_abajo.CSV',\n",
       " '290424_LIT921.1_1500ms_n8_potencia1c_abajo.CSV']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "archivos = os.listdir('D:\\OneDrive - UNICAN\\Escritorio\\Conchas\\CSV_nanobase')\n",
    "archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240424_LIT235.3_2000ms_n10_potencia1c_abajo.CSV: se descartaron 0 espectros saturados de un total de 1624 espectros.\n",
      "250424_LIT885.1_3000ms_n8_potencia1c_abajo.CSV: se descartaron 919 espectros saturados de un total de 1092 espectros.\n",
      "180424_LIT3_1000ms_n5_potencia1c_mapa_abajo.CSV: se descartaron 0 espectros saturados de un total de 4494 espectros.\n",
      "190424_LIT5_1000ms_n10_potencia1c_abajo.CSV: se descartaron 1 espectros saturados de un total de 4482 espectros.\n",
      "230424_LIT6_1000ms_n5_potencia1c_abajo.CSV: se descartaron 0 espectros saturados de un total de 4482 espectros.\n",
      "160424_LIT4_1000ms_n5_potenica1c_mapa_abajo.CSV: se descartaron 127 espectros saturados de un total de 4494 espectros.\n",
      "170424_LIT2_1000ms_n5_potencia1c_mapa_abajo.CSV: se descartaron 0 espectros saturados de un total de 4494 espectros.\n",
      "180424_LIT1_1000ms_n5_potencia1c_mapa_abajo.CSV: se descartaron 30 espectros saturados de un total de 4482 espectros.\n"
     ]
    }
   ],
   "source": [
    "LIT235_3 = procesarRaman_carpeta(['240424_LIT235.3_2000ms_n10_potencia1c_abajo.CSV'])\n",
    "LIT885_1 = procesarRaman_carpeta(['250424_LIT885.1_3000ms_n8_potencia1c_abajo.CSV'])\n",
    "LIT3 = procesarRaman_carpeta(['180424_LIT3_1000ms_n5_potencia1c_mapa_abajo.CSV'])\n",
    "LIT5 = procesarRaman_carpeta(['190424_LIT5_1000ms_n10_potencia1c_abajo.CSV'])\n",
    "LIT6 = procesarRaman_carpeta(['230424_LIT6_1000ms_n5_potencia1c_abajo.CSV'])\n",
    "LIT4 = procesarRaman_carpeta(['160424_LIT4_1000ms_n5_potenica1c_mapa_abajo.CSV'])\n",
    "LIT2 = procesarRaman_carpeta(['170424_LIT2_1000ms_n5_potencia1c_mapa_abajo.CSV'])\n",
    "LIT1 = procesarRaman_carpeta(['180424_LIT1_1000ms_n5_potencia1c_mapa_abajo.CSV'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
