{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesado Espectros Raman NANOBASE\n",
    "\n",
    "En este *Notebook* se realiza un preprocesado básico a los espectros Raman medidos con Nanobase. \n",
    "\n",
    "Los datos obtenidos de NanoBase corresponden a espectros en bruto a los que se les ha restado el *background*. El programa de medición no realiza ningun otro tipo de tratamiento a los datos. Por ello es necesario realizar diferentes pasos previos al análisis de los espectros:\n",
    "\n",
    "1. Whittaker Smoother: Método de suavizado aplicado a señales con la finalidad de reducir el ruido, conservando los picos caracteristicos del material. El método funciona minimizando la suma de las diferencias al cuadrado entre cada punto de datos y su estimación suavizada, sujeto a una restricción de que la segunda diferencia de los valores suavizados es pequeña. Esta restricción tiene el efecto de penalizar las variaciones en la tasa de cambio de los valores suavizados, resultando en una señal más suave.\n",
    "\n",
    "2. Baseline: \n",
    "\n",
    "Al contrario que en el preprocesado de la Sonda Raman de 532nm, no es necesario corregir el Background ya que el propio software de medida se encarga de hacerlo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de archivos CSV:  9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#%matplotlib inline\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import time\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, Normalizer\n",
    "from scipy.linalg import solveh_banded  # para hacer ALS\n",
    "from scipy import sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.signal import argrelextrema\n",
    "from itertools import product\n",
    "from scipy.signal import savgol_filter\n",
    "from functools import reduce\n",
    "\n",
    "# Directorio en el que se encuentran guardados los datos de Raman\n",
    "path_nanobase_mac = '/Users/drea/Library/CloudStorage/OneDrive-UNICAN/RamanNanoBase/CONCHAS_ARQ/CSV'\n",
    "path_nanobase_windows ='D:\\OneDrive - UNICAN\\RamanNanoBase\\CONCHAS_ARQ\\CSV'\n",
    "# Elegimos la ruta según el sistema operativo\n",
    "path_nanobase = path_nanobase_windows\n",
    "# Obtén la lista de archivos en la carpeta\n",
    "files = os.listdir(path_nanobase)\n",
    "# Filtrar solo los archivos CSV\n",
    "csv_files = [f for f in files if f.endswith('.CSV')]\n",
    "\n",
    "print(\"Número de archivos CSV: \", len(csv_files))\n",
    "\n",
    "# Copiamos los archivos a la carpeta contenedora. Esto se realiza para poder actualizar las nuevas medidas\n",
    "def copiar_archivos(origen, destino, archivos):\n",
    "    # Copiar cada archivo seleccionado a la carpeta de destino si no existe\n",
    "    for archivo in archivos:\n",
    "        ruta_origen = os.path.join(origen, archivo)\n",
    "        ruta_destino = os.path.join(destino, archivo)\n",
    "        if os.path.exists(ruta_origen):\n",
    "            shutil.copy2(ruta_origen, ruta_destino)\n",
    "            print(f\"Archivo {archivo} copiado correctamente.\")\n",
    "        else:\n",
    "            print(f\"El archivo {archivo} no existe en la carpeta de origen.\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "carpeta_origen = 'D:\\OneDrive - UNICAN\\RamanNanoBase\\CONCHAS_ARQ\\CSV'\n",
    "carpeta_destino = \"D:\\OneDrive - UNICAN\\Escritorio\\Conchas\\CSV_nanobase\"\n",
    "\n",
    "# copiar_archivos(carpeta_origen, carpeta_destino, csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de archivos CSV:  9\n"
     ]
    }
   ],
   "source": [
    "# Directorios de medida\n",
    "csv_files = os.listdir(carpeta_destino)\n",
    "print(\"Número de archivos CSV: \", len(csv_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhittakerSmoother(object):\n",
    "    def __init__(self, signal, smoothness_param, deriv_order=1):\n",
    "        self.y = signal\n",
    "        assert deriv_order > 0, 'deriv_order must be an int > 0'\n",
    "        # Compute the fixed derivative of identity (D).\n",
    "        d = np.zeros(deriv_order*2 + 1, dtype=int)\n",
    "        d[deriv_order] = 1\n",
    "        d = np.diff(d, n=\n",
    "        deriv_order)\n",
    "        n = self.y.shape[0]\n",
    "        k = len(d)\n",
    "        s = float(smoothness_param)\n",
    "        # Here be dragons: essentially we're faking a big banded matrix D,\n",
    "        # doing s * D.T.dot(D) with it, then taking the upper triangular bands.\n",
    "        diag_sums = np.vstack([\n",
    "            np.pad(s*np.cumsum(d[-i:]*d[:i]), ((k-i,0),), 'constant')\n",
    "            for i in range(1, k+1)])\n",
    "        upper_bands = np.tile(diag_sums[:,-1:], n)\n",
    "        upper_bands[:,:k] = diag_sums\n",
    "        for i,ds in enumerate(diag_sums):\n",
    "            upper_bands[i,-i-1:] = ds[::-1][:i+1]\n",
    "        self.upper_bands = upper_bands\n",
    "    def smooth(self, w):\n",
    "        foo = self.upper_bands.copy()\n",
    "        foo[-1] += w  # last row is the diagonal\n",
    "        return solveh_banded(foo, w * self.y, overwrite_ab=True, overwrite_b=True)\n",
    "\n",
    "def als_baseline(intensities, asymmetry_param=0.0001, smoothness_param=1e4,\n",
    "                 max_iters=20, conv_thresh=1e-6, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies the asymmetric least squares (ALS) method to fit the baseline.\n",
    "\n",
    "    Parameters:\n",
    "    - intensities (numpy.ndarray or pandas.Series): Numpy array or pandas Series representing the Raman intensities.\n",
    "    - asymmetry_param (float): Asymmetry parameter for the fit.\n",
    "    - smoothness_param (float): Smoothness parameter for the fit.\n",
    "    - max_iters (int): Maximum number of iterations.\n",
    "    - conv_thresh (float): Convergence threshold.\n",
    "    - verbose (bool): Flag to print debugging information.\n",
    "\n",
    "    Returns:\n",
    "    - baseline (numpy.ndarray): Numpy array representing the fitted baseline.\n",
    "    \"\"\"\n",
    "    if isinstance(intensities, pd.Series):\n",
    "        intensities = intensities.values\n",
    "\n",
    "    smoother = WhittakerSmoother(intensities, smoothness_param, deriv_order=2)\n",
    "    p = asymmetry_param\n",
    "    w = np.ones(intensities.shape[0])\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        z = smoother.smooth(w)\n",
    "        mask = intensities > z\n",
    "        new_w = p * mask + (1 - p) * (~mask)\n",
    "        conv = np.linalg.norm(new_w - w)\n",
    "        if verbose:\n",
    "            print(i + 1, conv)\n",
    "        if conv < conv_thresh:\n",
    "            break\n",
    "        w = new_w\n",
    "    else:\n",
    "        None\n",
    "        #print('ALS did not converge in %d iterations' % max_iters)\n",
    "    return z\n",
    "\n",
    "def subtract(df, df_background):\n",
    "    df_subtracted = df.sub(df_background.iloc[0], axis=1)\n",
    "    return df_subtracted\n",
    "\n",
    "def find_baseline(df, asymmetry_param=0.0001, smoothness_param=1e4,\n",
    "                 max_iters=20, conv_thresh=1e-6):\n",
    "    \"\"\"\n",
    "    Apply baseline correction to each row in a DataFrame.\n",
    "    Input:\n",
    "        df: DataFrame, contains the spectral data.\n",
    "    Output:\n",
    "        DataFrame, with baseline corrected for each spectrum.\n",
    "    \"\"\"\n",
    "    D0_BL = df.copy()\n",
    "    for i in range(len(df)):\n",
    "        # Apply ALS baseline correction to each row\n",
    "        bl = als_baseline(df.iloc[i, :].values, asymmetry_param, smoothness_param,\n",
    "                 max_iters, conv_thresh)\n",
    "        D0_BL.iloc[i, :] = bl\n",
    "    return D0_BL\n",
    "\n",
    "def correct_baseline(data, baseline):\n",
    "    \"\"\"\n",
    "    Corrects the baseline of the data.\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame or pandas.Series): DataFrame or Series containing the data.\n",
    "    - baseline (pandas.DataFrame or pandas.Series): DataFrame or Series with the baselines.\n",
    "    Returns:\n",
    "    - corrected_data (pandas.DataFrame): DataFrame with the corrected data.\n",
    "    \"\"\"\n",
    "    # Ensure that both data and baseline are DataFrames\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "         data = pd.DataFrame(data)\n",
    "    if not isinstance(baseline, pd.DataFrame):\n",
    "         baseline = pd.DataFrame(baseline)\n",
    "    # Perform baseline correction\n",
    "    \n",
    "    corrected_data = data - baseline\n",
    "\n",
    "    return corrected_data\n",
    "\n",
    "\n",
    "def normalize_spectra(X_filtered):\n",
    "    \"\"\"\n",
    "    Preprocessing of spectral data.\n",
    "    Input:\n",
    "        X_filtered: DataFrame, raw spectral data.\n",
    "    Output:\n",
    "        DataFrame, preprocessed spectral data.\n",
    "    \"\"\"\n",
    "    X_norm_list = []\n",
    "    for _, row in X_filtered.iterrows():\n",
    "        # Normalize each spectrum by dividing by the sum and multiplying by a constant\n",
    "        total = np.sum(row)\n",
    "        X_norm_list.append(pd.DataFrame([row / total * 2000]))\n",
    "    X_norm = pd.concat(X_norm_list, ignore_index=True)\n",
    "    # Apply Savitzky-Golay filter for smoothing\n",
    "    X_norm_flt = savgol_filter(X_norm, 21, 2)\n",
    "    X_norm_flt = pd.DataFrame(X_norm_flt, columns=X_filtered.columns)\n",
    "    '''# Perform baseline correction\n",
    "    bl = find_baseline(X_norm_flt)\n",
    "    X_norm_flt = X_norm_flt - bl'''\n",
    "    # Standardize the data using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    z_numpy = X_norm_flt.values\n",
    "    z_numpy_scaled_rows = scaler.fit_transform(z_numpy.T).T\n",
    "    X_norm_flt_stdz = pd.DataFrame(data=z_numpy_scaled_rows, columns=X_filtered.columns)\n",
    "    return X_norm_flt_stdz\n",
    "\n",
    "def SNV(input_data):\n",
    "    \"\"\"\n",
    "    Procesado de datos: SNV (Standard Normal Variate).\n",
    "    1) Media de cada channel --> axis=1 (todas las columnas).\n",
    "    2) Se le resta su media a cada \"channel\" --> axis=0 (todas las filas).\n",
    "    3) Divide cada channel por su StDev --> axis=1 (todas las columnas).\n",
    "    :parameter *input_data*: matriz de datos, en formato: \"channels(f) x time(c)\".\n",
    "    :return: matriz de datos, una vez normalizados\n",
    "    \"\"\"\n",
    "    data_12 = input_data.sub(input_data.mean(axis=1), axis=0)\n",
    "    data_snv = data_12.div(input_data.std(axis=1), axis=0)\n",
    "    return data_snv\n",
    "\n",
    "def cut_spectrum(df, min_wavelength, max_wavelength):\n",
    "    \"\"\"\n",
    "    Cuts the spectrum to a specific range.\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): DataFrame containing the spectrum.\n",
    "    - min_wavelength (int): Minimum wavelength.\n",
    "    - max_wavelength (int): Maximum wavelength.\n",
    "    Returns:\n",
    "    - df_cut (pandas.DataFrame): DataFrame containing the spectrum within the specified range.\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.astype(float) #si tus columnas deben ser números flotantes\n",
    "    df_cut = df.loc[:, (df.columns >= min_wavelength) & (df.columns <= max_wavelength)]\n",
    "    return df_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['160424_LIT4_1000ms_n5_potenica1c_mapa_abajo.CSV',\n",
       " '170424_LIT2_1000ms_n5_potencia1c_mapa_abajo.CSV',\n",
       " '180424_LIT1_1000ms_n5_potencia1c_mapa_abajo.CSV',\n",
       " '180424_LIT3_1000ms_n5_potencia1c_mapa_abajo.CSV',\n",
       " '190424_LIT5_1000ms_n10_potencia1c_abajo.CSV',\n",
       " '230424_LIT6_1000ms_n5_potencia1c_abajo.CSV',\n",
       " '240424_LIT235.3_2000ms_n10_potencia1c_abajo.CSV',\n",
       " '250424_LIT885.1_3000ms_n8_potencia1c_abajo.CSV',\n",
       " '290424_LIT921.1_1500ms_n8_potencia1c_abajo.CSV']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160424_LIT4_1000ms_n5_potenica1c_mapa_abajo.CSV\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>XINDEX</th>\n",
       "      <th>YINDEX</th>\n",
       "      <th>ZINDEX</th>\n",
       "      <th>-77.043</th>\n",
       "      <th>-74.805</th>\n",
       "      <th>-72.569</th>\n",
       "      <th>-70.332</th>\n",
       "      <th>...</th>\n",
       "      <th>3304.067</th>\n",
       "      <th>3305.232</th>\n",
       "      <th>3306.397</th>\n",
       "      <th>3307.567</th>\n",
       "      <th>3308.734</th>\n",
       "      <th>3309.900</th>\n",
       "      <th>3311.064</th>\n",
       "      <th>3312.227</th>\n",
       "      <th>3313.389</th>\n",
       "      <th>3314.551</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-187.014249</td>\n",
       "      <td>-51.936528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>316.4</td>\n",
       "      <td>338.6</td>\n",
       "      <td>326.8</td>\n",
       "      <td>320.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4474.6</td>\n",
       "      <td>4404.8</td>\n",
       "      <td>4346.4</td>\n",
       "      <td>4612.4</td>\n",
       "      <td>4417.6</td>\n",
       "      <td>4518.0</td>\n",
       "      <td>4493.4</td>\n",
       "      <td>4438.8</td>\n",
       "      <td>4383.6</td>\n",
       "      <td>4475.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-183.514249</td>\n",
       "      <td>-51.936528</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>312.8</td>\n",
       "      <td>299.8</td>\n",
       "      <td>312.8</td>\n",
       "      <td>321.6</td>\n",
       "      <td>...</td>\n",
       "      <td>4322.4</td>\n",
       "      <td>4244.4</td>\n",
       "      <td>4257.8</td>\n",
       "      <td>4252.6</td>\n",
       "      <td>4138.2</td>\n",
       "      <td>4296.6</td>\n",
       "      <td>4187.4</td>\n",
       "      <td>4315.8</td>\n",
       "      <td>4197.0</td>\n",
       "      <td>4162.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-180.014249</td>\n",
       "      <td>-51.936528</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>328.0</td>\n",
       "      <td>339.4</td>\n",
       "      <td>319.2</td>\n",
       "      <td>336.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5390.4</td>\n",
       "      <td>5342.6</td>\n",
       "      <td>5404.0</td>\n",
       "      <td>5395.6</td>\n",
       "      <td>5353.6</td>\n",
       "      <td>5278.6</td>\n",
       "      <td>5324.4</td>\n",
       "      <td>5412.8</td>\n",
       "      <td>5258.0</td>\n",
       "      <td>5287.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-176.514249</td>\n",
       "      <td>-51.936528</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>326.4</td>\n",
       "      <td>290.6</td>\n",
       "      <td>310.0</td>\n",
       "      <td>309.6</td>\n",
       "      <td>...</td>\n",
       "      <td>3448.4</td>\n",
       "      <td>3580.8</td>\n",
       "      <td>3538.8</td>\n",
       "      <td>3577.6</td>\n",
       "      <td>3466.8</td>\n",
       "      <td>3647.6</td>\n",
       "      <td>3439.2</td>\n",
       "      <td>3559.0</td>\n",
       "      <td>3518.6</td>\n",
       "      <td>3496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-173.014249</td>\n",
       "      <td>-51.936528</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>306.4</td>\n",
       "      <td>313.4</td>\n",
       "      <td>340.4</td>\n",
       "      <td>319.6</td>\n",
       "      <td>...</td>\n",
       "      <td>4960.2</td>\n",
       "      <td>4862.8</td>\n",
       "      <td>4820.2</td>\n",
       "      <td>4977.6</td>\n",
       "      <td>4862.6</td>\n",
       "      <td>4931.0</td>\n",
       "      <td>4844.8</td>\n",
       "      <td>4894.2</td>\n",
       "      <td>4775.6</td>\n",
       "      <td>4816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4489</th>\n",
       "      <td>169.985751</td>\n",
       "      <td>50.563472</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>319.6</td>\n",
       "      <td>313.0</td>\n",
       "      <td>354.4</td>\n",
       "      <td>318.4</td>\n",
       "      <td>...</td>\n",
       "      <td>3613.8</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>3751.2</td>\n",
       "      <td>3793.4</td>\n",
       "      <td>3620.0</td>\n",
       "      <td>3666.0</td>\n",
       "      <td>3663.8</td>\n",
       "      <td>3665.2</td>\n",
       "      <td>3662.8</td>\n",
       "      <td>3593.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4490</th>\n",
       "      <td>173.485751</td>\n",
       "      <td>50.563472</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>334.8</td>\n",
       "      <td>333.8</td>\n",
       "      <td>336.4</td>\n",
       "      <td>299.6</td>\n",
       "      <td>...</td>\n",
       "      <td>3157.6</td>\n",
       "      <td>3116.4</td>\n",
       "      <td>3076.4</td>\n",
       "      <td>3080.6</td>\n",
       "      <td>3110.0</td>\n",
       "      <td>3130.6</td>\n",
       "      <td>3135.0</td>\n",
       "      <td>3071.6</td>\n",
       "      <td>3130.8</td>\n",
       "      <td>2978.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4491</th>\n",
       "      <td>176.985751</td>\n",
       "      <td>50.563472</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>310.8</td>\n",
       "      <td>315.0</td>\n",
       "      <td>334.4</td>\n",
       "      <td>352.4</td>\n",
       "      <td>...</td>\n",
       "      <td>3411.8</td>\n",
       "      <td>3500.4</td>\n",
       "      <td>3447.0</td>\n",
       "      <td>3475.6</td>\n",
       "      <td>3441.2</td>\n",
       "      <td>3543.6</td>\n",
       "      <td>3544.6</td>\n",
       "      <td>3460.8</td>\n",
       "      <td>3442.4</td>\n",
       "      <td>3428.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4492</th>\n",
       "      <td>180.485751</td>\n",
       "      <td>50.563472</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>336.8</td>\n",
       "      <td>294.6</td>\n",
       "      <td>334.0</td>\n",
       "      <td>308.8</td>\n",
       "      <td>...</td>\n",
       "      <td>4396.2</td>\n",
       "      <td>4269.8</td>\n",
       "      <td>4347.6</td>\n",
       "      <td>4419.6</td>\n",
       "      <td>4302.4</td>\n",
       "      <td>4475.6</td>\n",
       "      <td>4374.2</td>\n",
       "      <td>4383.8</td>\n",
       "      <td>4370.6</td>\n",
       "      <td>4268.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4493</th>\n",
       "      <td>183.985751</td>\n",
       "      <td>50.563472</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>292.8</td>\n",
       "      <td>331.4</td>\n",
       "      <td>335.2</td>\n",
       "      <td>334.4</td>\n",
       "      <td>...</td>\n",
       "      <td>8930.0</td>\n",
       "      <td>9087.4</td>\n",
       "      <td>9127.2</td>\n",
       "      <td>9074.4</td>\n",
       "      <td>9026.4</td>\n",
       "      <td>8972.6</td>\n",
       "      <td>9088.0</td>\n",
       "      <td>8976.6</td>\n",
       "      <td>8990.2</td>\n",
       "      <td>9302.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4494 rows × 1937 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               X          Y  Z  XINDEX  YINDEX  ZINDEX  -77.043  -74.805  \\\n",
       "0    -187.014249 -51.936528  0       0       0       0    316.4    338.6   \n",
       "1    -183.514249 -51.936528  0       1       0       0    312.8    299.8   \n",
       "2    -180.014249 -51.936528  0       2       0       0    328.0    339.4   \n",
       "3    -176.514249 -51.936528  0       3       0       0    326.4    290.6   \n",
       "4    -173.014249 -51.936528  0       4       0       0    306.4    313.4   \n",
       "...          ...        ... ..     ...     ...     ...      ...      ...   \n",
       "4489  169.985751  50.563472  0     102      41       0    319.6    313.0   \n",
       "4490  173.485751  50.563472  0     103      41       0    334.8    333.8   \n",
       "4491  176.985751  50.563472  0     104      41       0    310.8    315.0   \n",
       "4492  180.485751  50.563472  0     105      41       0    336.8    294.6   \n",
       "4493  183.985751  50.563472  0     106      41       0    292.8    331.4   \n",
       "\n",
       "      -72.569  -70.332  ...  3304.067  3305.232  3306.397  3307.567  3308.734  \\\n",
       "0       326.8    320.0  ...    4474.6    4404.8    4346.4    4612.4    4417.6   \n",
       "1       312.8    321.6  ...    4322.4    4244.4    4257.8    4252.6    4138.2   \n",
       "2       319.2    336.0  ...    5390.4    5342.6    5404.0    5395.6    5353.6   \n",
       "3       310.0    309.6  ...    3448.4    3580.8    3538.8    3577.6    3466.8   \n",
       "4       340.4    319.6  ...    4960.2    4862.8    4820.2    4977.6    4862.6   \n",
       "...       ...      ...  ...       ...       ...       ...       ...       ...   \n",
       "4489    354.4    318.4  ...    3613.8    3693.0    3751.2    3793.4    3620.0   \n",
       "4490    336.4    299.6  ...    3157.6    3116.4    3076.4    3080.6    3110.0   \n",
       "4491    334.4    352.4  ...    3411.8    3500.4    3447.0    3475.6    3441.2   \n",
       "4492    334.0    308.8  ...    4396.2    4269.8    4347.6    4419.6    4302.4   \n",
       "4493    335.2    334.4  ...    8930.0    9087.4    9127.2    9074.4    9026.4   \n",
       "\n",
       "      3309.900  3311.064  3312.227  3313.389  3314.551  \n",
       "0       4518.0    4493.4    4438.8    4383.6    4475.2  \n",
       "1       4296.6    4187.4    4315.8    4197.0    4162.8  \n",
       "2       5278.6    5324.4    5412.8    5258.0    5287.8  \n",
       "3       3647.6    3439.2    3559.0    3518.6    3496.0  \n",
       "4       4931.0    4844.8    4894.2    4775.6    4816.0  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "4489    3666.0    3663.8    3665.2    3662.8    3593.2  \n",
       "4490    3130.6    3135.0    3071.6    3130.8    2978.4  \n",
       "4491    3543.6    3544.6    3460.8    3442.4    3428.6  \n",
       "4492    4475.6    4374.2    4383.8    4370.6    4268.2  \n",
       "4493    8972.6    9088.0    8976.6    8990.2    9302.0  \n",
       "\n",
       "[4494 rows x 1937 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_df_raw(folder_path, csv_files , cut= 'cut'):\n",
    "    dataframes = []\n",
    "    lista_archivos = []\n",
    "    lista_muestras = []\n",
    "\n",
    "    path = folder_path\n",
    "    for csv_file in csv_files:\n",
    "            csv_file_complete = os.path.join(path, csv_file)\n",
    "            df_pre = pd.read_csv(csv_file_complete, engine='python', sep=',', skiprows=14, decimal='.')\n",
    "            resultados = []\n",
    "            archivo= csv_file\n",
    "\n",
    "            # # Drop unwanted column\n",
    "            if 'Unnamed: 1937' in df_pre.columns:\n",
    "                df_pre = df_pre.drop(columns=['Unnamed: 1937'])\n",
    "\n",
    "            # Realizamos el preprocesado de los datos\n",
    "            # df_pre_plot = df_pre.copy()\n",
    "            # if cut == 'cut':\n",
    "            #     df_cut = cut_spectrum(df_pre, min_wavelength=179, max_wavelength=2800)\n",
    "            # else:\n",
    "            #     df_cut = df_pre.copy()\n",
    "            # df_find_baseline = find_baseline(df_cut)\n",
    "            # df_media_corrected = correct_baseline(df_cut, df_find_baseline)\n",
    "\n",
    "\n",
    "            # Podemos plotear el paso a paso para comprobar que se ha realizado correctamente\n",
    "            # df_pre_plot.columns = df_pre.columns.astype(float)\n",
    "            # df_pre_plot.mean(axis=0).plot()\n",
    "            # df_cut.mean(axis=0).plot()\n",
    "            # df_find_baseline.mean(axis=0).plot()\n",
    "            # df_media_corrected.mean(axis=0).plot()\n",
    "            print(archivo)\n",
    "\n",
    "            # df = df_media_corrected.copy()\n",
    "\n",
    "            # nombre_archivo = archivo # Nombre del archivo\n",
    "            # nombre_muestra = nombre_archivo.split(\"_\")[3] # Nombre de la muestra\n",
    "            # fecha_medida = nombre_archivo.split(\"_\")[0].split(\"-\")[0]  # Fecha de la medida\n",
    "            # hora_medida = nombre_archivo.split(\"_\")[0].split(\"-\")[1]    # Hora de la medida\n",
    "            # tipo_laser =   \"Nanobase\"\n",
    "            # tipo = 'raw'\n",
    "            # df.index = pd.MultiIndex.from_tuples([( tipo_laser, nombre_muestra, tipo,\n",
    "            #                                             fecha_medida,hora_medida, nombre_archivo, X, Y, ) for X, Y in df.index],\n",
    "            #                                       names=[ \"Instrumento\",\"Muestra\",\"Tipo\",  \"Fecha\",\"Hora\", \"Archivo\",\"X\", \"Y\"])\n",
    "\n",
    "            # Guardamos el nombre de los archivos y de las muestras en listas\n",
    "            # lista_archivos.append(nombre_archivo)\n",
    "            # lista_muestras.append(nombre_muestra)\n",
    "            # # Agrega el DataFrame a la lista\n",
    "            dataframes.append(df_pre)\n",
    "\n",
    "    # Concatena todos los DataFrames en uno solo\n",
    "    data = pd.concat(dataframes)\n",
    "    return data\n",
    "\n",
    "data = create_df_raw(carpeta_destino, ['160424_LIT4_1000ms_n5_potenica1c_mapa_abajo.CSV'], cut='cut')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhittakerSmoother(object):\n",
    "    def __init__(self, signal, smoothness_param, deriv_order=1):\n",
    "        self.y = signal\n",
    "        assert deriv_order > 0, 'deriv_order must be an int > 0'\n",
    "        # Compute the fixed derivative of identity (D).\n",
    "        d = np.zeros(deriv_order*2 + 1, dtype=int)\n",
    "        # d[deriv_order] = 1\n",
    "        d = np.diff(d, n=\n",
    "        deriv_order)\n",
    "        n = self.y.shape[0]\n",
    "        k = len(d)\n",
    "        s = float(smoothness_param)\n",
    "        # Here be dragons: essentially we're faking a big banded matrix D,\n",
    "        # doing s * D.T.dot(D) with it, then taking the upper triangular bands.\n",
    "        diag_sums = np.vstack([\n",
    "            np.pad(s*np.cumsum(d[-i:]*d[:i]), ((k-i,0),), 'constant')\n",
    "            for i in range(1, k+1)])\n",
    "        upper_bands = np.tile(diag_sums[:,-1:], n)\n",
    "        upper_bands[:,:k] = diag_sums\n",
    "        for i,ds in enumerate(diag_sums):\n",
    "            upper_bands[i,-i-1:] = ds[::-1][:i+1]\n",
    "        self.upper_bands = upper_bands\n",
    "    def smooth(self, w):\n",
    "        foo = self.upper_bands.copy()\n",
    "        foo[-1] += w  # last row is the diagonal\n",
    "        return solveh_banded(foo, w * self.y, overwrite_ab=True, overwrite_b=True)\n",
    "\n",
    "def als_baseline(intensities, asymmetry_param=0.0001, smoothness_param=1e4,\n",
    "                 max_iters=20, conv_thresh=1e-6, verbose=False):\n",
    "    \"\"\"\n",
    "    Applies the asymmetric least squares (ALS) method to fit the baseline.\n",
    "\n",
    "    Parameters:\n",
    "    - intensities (numpy.ndarray or pandas.Series): Numpy array or pandas Series representing the Raman intensities.\n",
    "    - asymmetry_param (float): Asymmetry parameter for the fit.\n",
    "    - smoothness_param (float): Smoothness parameter for the fit.\n",
    "    - max_iters (int): Maximum number of iterations.\n",
    "    - conv_thresh (float): Convergence threshold.\n",
    "    - verbose (bool): Flag to print debugging information.\n",
    "\n",
    "    Returns:\n",
    "    - baseline (numpy.ndarray): Numpy array representing the fitted baseline.\n",
    "    \"\"\"\n",
    "    if isinstance(intensities, pd.Series):\n",
    "        intensities = intensities.values\n",
    "\n",
    "    smoother = WhittakerSmoother(intensities, smoothness_param, deriv_order=2)\n",
    "    p = asymmetry_param\n",
    "    w = np.ones(intensities.shape[0])\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        z = smoother.smooth(w)\n",
    "        mask = intensities > z\n",
    "        new_w = p * mask + (1 - p) * (~mask)\n",
    "        conv = np.linalg.norm(new_w - w)\n",
    "\n",
    "        if verbose:\n",
    "            print(i + 1, conv)\n",
    "\n",
    "        if conv < conv_thresh:\n",
    "            break\n",
    "\n",
    "        w = new_w\n",
    "    else:\n",
    "        # print('ALS did not converge in %d iterations' % max_iters)\n",
    "        pass\n",
    "\n",
    "    return z\n",
    "\n",
    "\n",
    "\n",
    "def find_baseline(df):\n",
    "    \"\"\"\n",
    "    Apply baseline correction to each row in a DataFrame.\n",
    "    Input:\n",
    "        df: DataFrame, contains the spectral data.\n",
    "    Output:\n",
    "        DataFrame, with baseline corrected for each spectrum.\n",
    "    \"\"\"\n",
    "    D0_BL = df.copy()\n",
    "    for i in range(len(df)):\n",
    "        # Apply ALS baseline correction to each row\n",
    "        bl = als_baseline(df.iloc[i, :].values)\n",
    "        D0_BL.iloc[i, :] = bl\n",
    "    return D0_BL\n",
    "\n",
    "def remove_background(df):\n",
    "    D0_BG = df.copy()\n",
    "\n",
    "    return D0_BG\n",
    "\n",
    "def correct_baseline(data, baseline):\n",
    "    \"\"\"\n",
    "    Corrects the baseline of the data.\n",
    "    Parameters:\n",
    "    - data (pandas.DataFrame or pandas.Series): DataFrame or Series containing the data.\n",
    "    - baseline (pandas.DataFrame or pandas.Series): DataFrame or Series with the baselines.\n",
    "    Returns:\n",
    "    - corrected_data (pandas.DataFrame): DataFrame with the corrected data.\n",
    "    \"\"\"\n",
    "    # Ensure that both data and baseline are DataFrames\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    if not isinstance(baseline, pd.DataFrame):\n",
    "        baseline = pd.DataFrame(baseline)\n",
    "\n",
    "    # Perform baseline correction\n",
    "    corrected_data = data.values - baseline.values\n",
    "\n",
    "    # Convert the result back to a DataFrame while maintaining the same column names\n",
    "    corrected_data = pd.DataFrame(corrected_data, columns=data.columns).copy()\n",
    "\n",
    "    return corrected_data\n",
    "\n",
    "\n",
    "def normalize_spectra(X_filtered):\n",
    "    \"\"\"\n",
    "    Preprocessing of spectral data.\n",
    "    Input:\n",
    "        X_filtered: DataFrame, raw spectral data.\n",
    "    Output:\n",
    "        DataFrame, preprocessed spectral data.\n",
    "    \"\"\"\n",
    "    X_norm_list = []\n",
    "    for _, row in X_filtered.iterrows():\n",
    "        # Normalize each spectrum by dividing by the sum and multiplying by a constant\n",
    "        total = np.sum(row)\n",
    "        X_norm_list.append(pd.DataFrame([row / total * 2000]))\n",
    "\n",
    "    X_norm = pd.concat(X_norm_list, ignore_index=True)\n",
    "\n",
    "    # Apply Savitzky-Golay filter for smoothing\n",
    "    X_norm_flt = savgol_filter(X_norm, 21, 2)\n",
    "    X_norm_flt = pd.DataFrame(X_norm_flt, columns=X_filtered.columns)\n",
    "\n",
    "    '''# Perform baseline correction\n",
    "    bl = find_baseline(X_norm_flt)\n",
    "    X_norm_flt = X_norm_flt - bl'''\n",
    "\n",
    "    # Standardize the data using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    z_numpy = X_norm_flt.values\n",
    "    z_numpy_scaled_rows = scaler.fit_transform(z_numpy.T).T\n",
    "    X_norm_flt_stdz = pd.DataFrame(data=z_numpy_scaled_rows, columns=X_filtered.columns)\n",
    "\n",
    "    return X_norm_flt_stdz\n",
    "\n",
    "\n",
    "\n",
    "def procesarRamanNanobase(file):\n",
    "    \n",
    "    df = pd.read_csv(file, engine='python', sep=',', skiprows=14, decimal='.')\n",
    "    resultados = []\n",
    "    archivo= file\n",
    "\n",
    "    # # Drop unwanted column\n",
    "    if 'Unnamed: 1937' in df.columns:\n",
    "            df = df.drop(columns=['Unnamed: 1937'])\n",
    "\n",
    "    # Store coordinates\n",
    "    coordenadas = df.iloc[:, 0:2].T\n",
    "        \n",
    "    # Select only ramanshift columns\n",
    "    df2 = df.iloc[:, 7:]\n",
    "\n",
    "    # Create a DataFrame for Raman shift values\n",
    "    ramanshitfs = pd.DataFrame(df2.columns.astype(float), columns=['Raman shift'])\n",
    "\n",
    "    # Select Raman spectra data within the desired range\n",
    "    D = df2.loc[:, (df2.columns.astype(float) >= 500) & (df2.columns.astype(float) <=2000)]\n",
    "\n",
    "    # Filter Raman shift values within the desired range\n",
    "    ramanshitfs = ramanshitfs.loc[(ramanshitfs['Raman shift'] >= 500) & (ramanshitfs['Raman shift'] <= 2000)]\n",
    "\n",
    "\n",
    "    #Filtrado de espectros saturados\n",
    "    # guardado del número total de filas\n",
    "    total_rows = len(D)\n",
    "    # Descartar las filas donde cualquier valor supera 65000\n",
    "    D = D[~(D > 65000).any(axis=1)]\n",
    "\n",
    "    # Guardar el número de filas después de descartar\n",
    "    remaining_rows = len(D)\n",
    "    # Calcular e imprimir el número de filas descartadas\n",
    "    discarded_rows = total_rows - remaining_rows\n",
    "    print(f'{archivo}: se descartaron {discarded_rows} espectros saturados de un total de {total_rows} espectros.')\n",
    "    \n",
    "    # Find baseline of Raman spectra data\n",
    "    D_BL = find_baseline(D)\n",
    "    # Correct baseline of Raman spectra data\n",
    "    D_BC =correct_baseline(D, D_BL)\n",
    "    # Normalize Raman spectra data\n",
    "    D_PP = normalize_spectra(D_BC)\n",
    "\n",
    "    df_final = pd.concat([coordenadas.T, D_PP], axis=1)\n",
    "    df_final.set_index(['X', 'Y'], inplace=True)\n",
    "\n",
    "    tipo_laser = \"Nanobase\"\n",
    "    nombre_archivo = archivo.split('\\\\')[-1].split('.')[0]\n",
    "    nombre_muestra = nombre_archivo.split('_')[1]\n",
    "    fecha_medida = nombre_archivo.split('_')[0]\n",
    "    hora_medida = nombre_archivo.split(\"_\")[0].split(\"-\")[1]\n",
    "    tipo = 'raw'\n",
    "    df_final.index = pd.MultiIndex.from_tuples([( tipo_laser, nombre_muestra, tipo,\n",
    "                                                        fecha_medida,hora_medida, nombre_archivo, X, Y, ) for X, Y in df_final.index],\n",
    "                                                  names=[ \"Instrumento\",\"Muestra\",\"Tipo\",  \"Fecha\",\"Hora\", \"Archivo\",\"X\", \"Y\"])\n",
    "    \n",
    "    # Append processed data to the results list\n",
    "    resultados.append((archivo, coordenadas, D, D_BL, D_PP, ramanshitfs))\n",
    "\n",
    "    return df_final, resultados\n",
    "\n",
    "\n",
    "def procesarRamanSonda(file):\n",
    "    resultados_sonda =[]\n",
    "    archivo = file\n",
    "\n",
    "    df_sonda = pd.read_csv(file)\n",
    "    # Store coordinates\n",
    "    df_sonda_reset = df_sonda.reset_index()\n",
    "    coordenadas_sonda = df_sonda_reset.iloc[:, :2]\n",
    "    coordenadas_sonda.columns = ['X', 'Y']\n",
    "    coordenadas_sonda = coordenadas_sonda.T\n",
    "            \n",
    "    df2_sonda = df_sonda.iloc[:, 3:]\n",
    "\n",
    "    # Create a DataFrame for Raman shift values\n",
    "    ramanshitfs_sonda = pd.DataFrame(df2_sonda.columns.astype(float), columns=['Raman shift'])\n",
    "    # Select Raman spectra data within the desired range\n",
    "    D_sonda = df2_sonda.loc[:, (df2_sonda.columns.astype(float) >= 500.0) & (df2_sonda.columns.astype(float) <= 2000.0)]\n",
    "    # Filter Raman shift values within the desired range\n",
    "    ramanshitfs_sonda = ramanshitfs_sonda.loc[(ramanshitfs_sonda['Raman shift'] >= 500) & (ramanshitfs_sonda['Raman shift'] <= 2000)]\n",
    "    #Filtrado de espectros saturados\n",
    "    total_rows_sonda = len(D_sonda)\n",
    "    # Descartar las filas donde cualquier valor supera 65000\n",
    "    D_sonda = D_sonda[~(D_sonda > 65000).any(axis=1)]\n",
    "    # Guardar el número de filas después de descartar\n",
    "    remaining_rows_sonda = len(D_sonda)\n",
    "    # Calcular e imprimir el número de filas descartadas\n",
    "    discarded_rows_sonda = total_rows_sonda - remaining_rows_sonda\n",
    "\n",
    "    archivo_sonda ='Sonda'\n",
    "    print(f'{archivo_sonda}: se descartaron {discarded_rows_sonda} espectros saturados de un total de {total_rows_sonda} espectros.')\n",
    "\n",
    "        # Removing Background\n",
    "    D_BG_sonda = remove_background(D_sonda)\n",
    "    # Find baseline of Raman spectra data\n",
    "    D_BL_sonda = find_baseline(D_BG_sonda)\n",
    "    # Correct baseline of Raman spectra data\n",
    "    D_BC_sonda = correct_baseline(D_sonda, D_BL_sonda)\n",
    "    # Normalize Raman spectra data\n",
    "    D_PP_sonda = normalize_spectra(D_BC_sonda)\n",
    "\n",
    "    \n",
    "    df_sonda_final = pd.concat([coordenadas_sonda.T, D_PP_sonda], axis=1)\n",
    "    df_sonda_final.set_index(['X', 'Y'], inplace=True)\n",
    "\n",
    "    resultados_sonda.append((archivo, coordenadas_sonda, D_sonda, D_BL_sonda, D_PP_sonda, ramanshitfs_sonda))\n",
    "\n",
    "    nombre_archivo = archivo.split(\"\\\\\")[-2] # Nombre del archivo\n",
    "    nombre_muestra = nombre_archivo.split(\"_\")[3] # Nombre de la muestra\n",
    "    fecha_medida = nombre_archivo.split(\"_\")[0].split(\"-\")[0]  # Fecha de la medida\n",
    "    hora_medida = nombre_archivo.split(\"_\")[0].split(\"-\")[1]    # Hora de la medida\n",
    "    tipo_laser =   f\"Sonda {nombre_archivo.split('_')[2]}\"\n",
    "    tipo = 'raw'\n",
    "    df_sonda_final.index = pd.MultiIndex.from_tuples([( tipo_laser, nombre_muestra, tipo,\n",
    "                                                        fecha_medida,hora_medida, nombre_archivo, X, Y, ) for X, Y in df_sonda_final.index],\n",
    "                                                  names=[ \"Instrumento\",\"Muestra\",\"Tipo\",  \"Fecha\",\"Hora\", \"Archivo\",\"X\", \"Y\"])\n",
    "    return df_sonda_final, resultados_sonda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
